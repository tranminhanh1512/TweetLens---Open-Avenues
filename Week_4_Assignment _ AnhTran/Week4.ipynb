{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate 10,000 tweets using Transformer\n",
    "I generate 1,000 tweets 10 times because I got an error when I tried to generate 10,000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "def generate_tweets(prompt, num_tweets = 5):\n",
    "    return generator(prompt, num_return_sequences = num_tweets)\n",
    "\n",
    "prompt = \"Generate a tweet about tech layoff\"\n",
    "\n",
    "synthetic_tweets = generate_tweets(prompt, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write the generated tweets to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Write tweets to a CSV file\n",
    "csv_filename = \"generated_tweets.csv\"\n",
    "with open(csv_filename, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for tweet in synthetic_tweets:\n",
    "        writer.writerow([tweet[\"generated_text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine the generated tweets and the tweets from Twitter API last week\n",
    "I also remove the prompt \"Generate a tweet about tech layoff\" in the generated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# Combine 2 df, one is retrived from Twitter API, one is created with Transformer\n",
    "df1 = pd.read_csv('generated_tweets.csv')\n",
    "df1['Tweet'] = df1['Tweet'].apply(lambda x: re.sub(r'Generate a tweet about tech layoff[^\\w\\s]*', '', str(x)))  \n",
    "df2 = pd.read_csv('tweets.csv')\n",
    "df2_tweets = df2[['text']].rename(columns={'text': 'Tweet'})\n",
    "df_combined = pd.concat([df1, df2_tweets], ignore_index=True)\n",
    "df_combined.to_csv('combined_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techinques that I use to do the text preprocessing:\n",
    "- Text normalization (lowercasing)\n",
    "- URL removal\n",
    "- HTML tag removal\n",
    "- Punctuation removal\n",
    "- Whitespace management\n",
    "- Alphanumeric word removal\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tranminhanh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tranminhanh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the tokenizer and lemmatizer\n",
    "wpt = WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def normalized_text(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "        text = re.sub(r'<.*?>+', ' ', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "        tokens = wpt.tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Lemmatize tokens without stemming\n",
    "        lemma_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "        cleaned_text = ' '.join(lemma_tokens)\n",
    "        return cleaned_text\n",
    "    return ''  # If it's not a string, return an empty string\n",
    "\n",
    "data = pd.read_csv(\"combined_tweets.csv\")\n",
    "data['Tweet'] = data['Tweet'].apply(normalized_text)\n",
    "\n",
    "# Save to new csv file\n",
    "data.to_csv(\"cleaned_tweets.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
